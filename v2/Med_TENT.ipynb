{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 구글 드라이브 마운트"
      ],
      "metadata": {
        "id": "CZ1FaNCJBCvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehgj-XfdqDrb",
        "outputId": "07223fab-e3e6-44e4-c138-5ada81819f46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "### 필요한 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/greenDevil/진짜 쓸 거')\n",
        "### Required Packages\n",
        "from termcolor import colored\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pickle as pkl\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import tqdm\n",
        "import time\n",
        "import transformers\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "%matplotlib inline\n",
        "use_cuda = torch.cuda.is_available()\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "import copy\n",
        "from sklearn.metrics import average_precision_score\n",
        "import medtent # 우리가 제안하는 SFDA 기법인 Med-TENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "### pickle 파일 업로드\n",
        "\n",
        "The pickled list is a list of lists where each sublist represent a patient record that looks like\n",
        "[pt_id,label, seq_list , segment_list ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ovXxiY6Vp5g-"
      },
      "outputs": [],
      "source": [
        "test_f2=pkl.load( open('/content/drive/MyDrive/greenDevil/진짜 쓸 거/labeled_data_v2/my_bertft.test_175_sepsis.pkl', 'rb'), encoding='bytes')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 필요한 함수 정의"
      ],
      "metadata": {
        "id": "aY4fAvD7gnmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YCt1zb9Yp5g-"
      },
      "outputs": [],
      "source": [
        "### Below are key functions for  Data prepartion ,formating input data into features, and model defintion\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "\n",
        "\n",
        "\n",
        "def convert_EHRexamples_to_features(examples,max_seq_length):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        feature = convert_singleEHR_example(ex_index, example, max_seq_length)\n",
        "        features.append(feature)\n",
        "    return features\n",
        "\n",
        "### EHR version\n",
        "\n",
        "def convert_singleEHR_example(ex_index, example, max_seq_length):\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        is_real_example=False)\n",
        "\n",
        "    input_ids=example[2]\n",
        "    segment_ids=example[3]\n",
        "    label_id=example[1]\n",
        "\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "\n",
        "  # LR 5/13 Left Truncate longer sequence\n",
        "    while len(input_ids) > max_seq_length:\n",
        "        input_ids= input_ids[-max_seq_length:]\n",
        "        input_mask= input_mask[-max_seq_length:]\n",
        "        segment_ids= segment_ids[-max_seq_length:]\n",
        "\n",
        "\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "\n",
        "    feature =[input_ids,input_mask,segment_ids,label_id,True]\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sto1L9T2p5g_"
      },
      "outputs": [],
      "source": [
        "class BERTdataEHR(Dataset):\n",
        "    def __init__(self, Features):\n",
        "\n",
        "        self.data= Features\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx, seeDescription = False):\n",
        "\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "#customized parts for EHRdataloader\n",
        "def my_collate(batch):\n",
        "        all_input_ids = []\n",
        "        all_input_mask = []\n",
        "        all_segment_ids = []\n",
        "        all_label_ids = []\n",
        "\n",
        "        for feature in batch:\n",
        "            all_input_ids.append(feature[0])\n",
        "            all_input_mask.append(feature[1])\n",
        "            all_segment_ids.append(feature[2])\n",
        "            all_label_ids.append(feature[3])\n",
        "        return [all_input_ids, all_input_mask,all_segment_ids,all_label_ids]\n",
        "\n",
        "\n",
        "class BERTdataEHRloader(DataLoader):\n",
        "    def __init__(self, dataset, batch_size=128, shuffle=False, sampler=None, batch_sampler=None,\n",
        "                 num_workers=0, collate_fn=my_collate, pin_memory=False, drop_last=False,\n",
        "                 timeout=0, worker_init_fn=None):\n",
        "        DataLoader.__init__(self, dataset, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None,\n",
        "                 num_workers=0, collate_fn=my_collate, pin_memory=False, drop_last=False,\n",
        "                 timeout=0, worker_init_fn=None)\n",
        "        self.collate_fn = collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcd9W_RAp5g_"
      },
      "source": [
        "##### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4AFx-Ueep5g_"
      },
      "outputs": [],
      "source": [
        "class EHR_BERT_LR(nn.Module):\n",
        "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False ,time=False, preTrainEmb=''):\n",
        "        super(EHR_BERT_LR, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout_r = dropout_r\n",
        "        self.cell_type = cell_type\n",
        "        self.preTrainEmb=preTrainEmb\n",
        "        self.time=time\n",
        "\n",
        "        if bi: self.bi=2\n",
        "        else: self.bi=1\n",
        "\n",
        "        config = BertConfig.from_json_file(\"/content/drive/MyDrive/greenDevil/진짜 쓸 거/hospital73_pretraining_v2/config.json\")\n",
        "        self.PreBERTmodel = BertForSequenceClassification(config)\n",
        "        self.PreBERTmodel.load_state_dict(torch.load(\"/content/drive/MyDrive/greenDevil/진짜 쓸 거/converted_pytorch_model.bin\", map_location='cpu'), strict=False)\n",
        "\n",
        "        # self.PreBERTmodel=BertForSequenceClassification.from_pretrained(\"pretrained_py_models/45M_chk\")\n",
        "        if use_cuda:\n",
        "           self.PreBERTmodel.cuda()\n",
        "        input_size=self.PreBERTmodel.bert.config.vocab_size\n",
        "        self.in_size= self.PreBERTmodel.bert.config.hidden_size\n",
        "\n",
        "        self.dropout = nn.Dropout(p=self.dropout_r)\n",
        "        self.out = nn.Linear(self.in_size,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax=nn.Softmax()\n",
        "        if use_cuda:\n",
        "            self.flt_typ=torch.cuda.FloatTensor\n",
        "            self.lnt_typ=torch.cuda.LongTensor\n",
        "        else:\n",
        "            self.lnt_typ=torch.LongTensor\n",
        "            self.flt_typ=torch.FloatTensor\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        token_t=torch.from_numpy(np.asarray(sequence[0],dtype=int)).type(self.lnt_typ)\n",
        "        seg_t=torch.from_numpy(np.asarray(sequence[2],dtype=int)).type(self.lnt_typ)\n",
        "        Label_t=torch.from_numpy(np.asarray(sequence[3],dtype=int)).type(self.lnt_typ)\n",
        "        Bert_out=self.PreBERTmodel.bert(input_ids=token_t, attention_mask=torch.from_numpy(np.asarray(sequence[1],dtype=int)).type(self.lnt_typ),\n",
        "                                    token_type_ids=seg_t)\n",
        "        output=self.sigmoid(self.out(Bert_out[1]))\n",
        "\n",
        "        return output.squeeze(),Label_t.type(self.flt_typ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "la2_m3qHp5hA"
      },
      "outputs": [],
      "source": [
        "def timeSince(since):\n",
        "   now = time.time()\n",
        "   s = now - since\n",
        "   m = math.floor(s / 60)\n",
        "   s -= m * 60\n",
        "   return '%dm %ds' % (m, s)\n",
        "\n",
        "def trainsample(sample, model, optimizer, criterion = nn.BCELoss()):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    output,label_tensor = model(sample)\n",
        "    #label_tensor = smooth_labels(label_tensor, 0.1)\n",
        "    loss = criterion(output, label_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return output, loss.item()\n",
        "\n",
        "\n",
        "#train with loaders\n",
        "\n",
        "def trainbatches(mbs_list, model, optimizer, shuffle = True):\n",
        "    current_loss = 0\n",
        "    all_losses =[]\n",
        "    plot_every = 5\n",
        "    n_iter = 0\n",
        "    if shuffle:\n",
        "        random.shuffle(mbs_list)\n",
        "    for i,batch in enumerate(mbs_list):\n",
        "        output, loss = trainsample(batch, model, optimizer, criterion = nn.BCELoss())\n",
        "        current_loss += loss\n",
        "        n_iter +=1\n",
        "\n",
        "        if n_iter % plot_every == 0:\n",
        "            all_losses.append(current_loss/plot_every)\n",
        "            current_loss = 0\n",
        "    return current_loss, all_losses\n",
        "\n",
        "\n",
        "def calculate_auc(model, mbs_list, shuffle = False):\n",
        "    model.eval()\n",
        "    y_real =[]\n",
        "    y_hat= []\n",
        "    if shuffle:\n",
        "        random.shuffle(mbs_list)\n",
        "    for i,batch in enumerate(mbs_list):\n",
        "        output,label_tensor = model(batch)\n",
        "        y_hat.extend(output.cpu().data.view(-1).numpy())\n",
        "        y_real.extend(label_tensor.cpu().data.view(-1).numpy())\n",
        "    auc = roc_auc_score(y_real, y_hat)\n",
        "    ap = average_precision_score(y_real, y_hat)\n",
        "\n",
        "    return auc, ap, y_real, y_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_from_mycollate(batch, use_cuda=False, num_labels=1):\n",
        "    \"\"\"\n",
        "    my_collate가 반환한 형태:\n",
        "      [all_input_ids, all_input_mask, all_segment_ids, all_label_ids]\n",
        "    그런데 외부에서 한 번 더 래핑되어 [[...]] 형태로 올 수 있어 그리면 한 겹 벗겨줌\n",
        "    반환: (input_ids[ B,L ] Long, attention_mask[ B,L ] Long,\n",
        "           token_type_ids[ B,L ] Long, labels[ B ] Float(Long))\n",
        "    \"\"\"\n",
        "    # 한 겹 더 싸여 있으면 풀기\n",
        "    if isinstance(batch, (list, tuple)) and len(batch) == 1 and isinstance(batch[0], (list, tuple)):\n",
        "        batch = batch[0]\n",
        "\n",
        "    ids_li, mask_li, seg_li, lab_li = batch  # 4개 리스트\n",
        "    # 리스트/넘파이 → [B,L] LongTensor\n",
        "    input_ids      = torch.as_tensor(np.asarray(ids_li),  dtype=torch.long)\n",
        "    attention_mask = torch.as_tensor(np.asarray(mask_li), dtype=torch.long)\n",
        "    token_type_ids = torch.as_tensor(np.asarray(seg_li),  dtype=torch.long)\n",
        "\n",
        "    # 라벨 → [B]\n",
        "    labels = torch.as_tensor(np.asarray(lab_li))\n",
        "    labels = labels.float() if num_labels == 1 else labels.long()\n",
        "\n",
        "    if use_cuda:\n",
        "        input_ids = input_ids.cuda(non_blocking=True)\n",
        "        attention_mask = attention_mask.cuda(non_blocking=True)\n",
        "        token_type_ids = token_type_ids.cuda(non_blocking=True)\n",
        "        labels = labels.cuda(non_blocking=True)\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, labels"
      ],
      "metadata": {
        "id": "mfY3Z-kDpwmT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Med-TENT 적용"
      ],
      "metadata": {
        "id": "iLB2sLtxugMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Med-TENT version\n",
        "def new_epochs_run(make_model_fn, checkpoint_path, loader, lr, use_cuda=False, num_labels=1):\n",
        "    \"\"\"\n",
        "    make_model_fn: 동일 구조 모델 생성 함수\n",
        "    checkpoint_path: fine-tuned best state_dict 경로\n",
        "    loader: test2 DataLoader (my_collate 기반)\n",
        "    num_labels: 1(시그모이드) 또는 2(소프트맥스)\n",
        "    \"\"\"\n",
        "    def disable_dropout(m):\n",
        "      for name, child in m.named_children():\n",
        "        if isinstance(child, torch.nn.Dropout):\n",
        "            setattr(m, name, torch.nn.Identity())\n",
        "        else:\n",
        "            disable_dropout(child)\n",
        "\n",
        "    # 동일 가중치 모델 다시 로드 (적응 누적 방지)\n",
        "    ehr = make_model_fn()\n",
        "    state = torch.load(checkpoint_path, map_location=(\"cuda\" if use_cuda else \"cpu\"))\n",
        "    ehr.load_state_dict(state)\n",
        "    if use_cuda:\n",
        "        ehr.cuda()\n",
        "\n",
        "    # Med-TENT 셋업\n",
        "    base = ehr.PreBERTmodel\n",
        "    disable_dropout(base)\n",
        "    base = medtent.configure_model(base)                # LN만 requires_grad=True\n",
        "    ln_params, ln_names = medtent.collect_params(base, top_k=4, include_classifier_bias=False)  # bias update\n",
        "    # optimizer = optim.Adam(ln_params, lr=lr)\n",
        "    optimizer = optim.SGD(\n",
        "      ln_params,\n",
        "      lr=1e-3,\n",
        "      momentum=0.9,\n",
        "      nesterov=True,\n",
        "      weight_decay=0.0\n",
        "    )\n",
        "\n",
        "    model2 = medtent.MedTent(base, optimizer, steps=1, episodic=True)\n",
        "\n",
        "    y_true, y_hat = [], []\n",
        "    base.train()  # test-time에도 train 모드 유지\n",
        "\n",
        "    for batch in loader:\n",
        "        # my_collate → [all_ids, all_mask, all_segs, all_labels] 형태를 텐서로 정규화\n",
        "        input_ids, attention_mask, token_type_ids, labels = unpack_from_mycollate(\n",
        "            batch, use_cuda=use_cuda, num_labels=2\n",
        "        )\n",
        "        batch_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        outputs = model2(batch_dict)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "\n",
        "        if logits.ndim == 2 and logits.shape[1] == 1:\n",
        "            #sigmoid\n",
        "            probs = 1.0 / (1.0 + np.exp(-logits[:, 0]))\n",
        "        else:\n",
        "            #softmax\n",
        "            e = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
        "            probs = (e / e.sum(axis=1, keepdims=True))[:, 1]\n",
        "\n",
        "        y_hat.extend(probs.tolist())\n",
        "        y_true.extend(labels.detach().cpu().numpy().reshape(-1).tolist())\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_hat)\n",
        "    ap = average_precision_score(y_true, y_hat)\n",
        "    return auc, ap, np.array(y_true), np.array(y_hat)"
      ],
      "metadata": {
        "id": "mzN-qaPtY8Xb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "test_features2 = convert_EHRexamples_to_features(test_f2, MAX_SEQ_LENGTH)\n",
        "test2 = BERTdataEHR(test_features2)\n",
        "test_mbs2 = list(BERTdataEHRloader(test2, batch_size = BATCH_SIZE))"
      ],
      "metadata": {
        "id": "vE2Fwsm6dwPk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best 모델 state_dict 저장 경로 (epochs_run에서 저장한 경로 사용)\n",
        "ckpt_path = '/content/drive/MyDrive/greenDevil/진짜 쓸 거/test_LR_Bert_BiGRU_FCfirst_runEHRmodel.st'\n",
        "\n",
        "# test2(target domain)에 Med-TENT 적용하여 재평가\n",
        "make_model = lambda: EHR_BERT_LR(input_size= 90000, embed_dim=192, hidden_size=192)\n",
        "test2_auc_tent, test2_auprc_tent, y_true2, y_hat2 = new_epochs_run(\n",
        "    make_model_fn=make_model,\n",
        "    checkpoint_path=ckpt_path,\n",
        "    loader=test_mbs2,\n",
        "    lr=5e-3,\n",
        "    use_cuda=use_cuda,\n",
        "    num_labels=1\n",
        ")\n",
        "\n",
        "# print(f\"[Baseline] Test2 AUC: {desc3[('Test_AUC2', 'mean')].iloc[0]:.4f}\")\n",
        "# print(f\"[Baseline] Test2 AUPRC: {desc3[('Test_AUPRC2', 'mean')].iloc[0]:.4f}\")\n",
        "print(f\"[Apply Med-TENT] Test2 AUC: {test2_auc_tent:.4f}\")\n",
        "print(f\"[Apply Med-TENT] Test2 AUPRC: {test2_auprc_tent:.4f}\")"
      ],
      "metadata": {
        "id": "s1W1BjHJCA6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b16721-2569-46d4-fb3a-c5a0c6ba43b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Apply Med-TENT] Test2 AUC: 0.6021\n",
            "[Apply Med-TENT] Test2 AUPRC: 0.3603\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "py_37_env",
      "language": "python",
      "name": "py_37_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}